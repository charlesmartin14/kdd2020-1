\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sxn:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{weightwatcher_package}
\citation{kdd20_sub_repo_anonymized}
\citation{EB01_BOOK,MM17_TR,BKPx20}
\citation{MM17_TR,MM18_TR,MM19_HTSR_ICML,MM19_KDD,MM20_SDM}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{EB01_BOOK,BKPx20}
\citation{BouchaudPotters03,SornetteBook,BP11,bun2017}
\citation{NTS15,BFT17_TR,LMBx18_TR}
\@writefile{toc}{\contentsline {paragraph}{The WeightWatcher Tool.}{2}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{Organization of this paper.}{2}{section*.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Related Work}{2}{section.2}}
\newlabel{sxn:background}{{2}{2}{Background and Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Statistical mechanics theory for DNNs.}{2}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Norm-based capacity control theory.}{2}{section*.6}}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{weightwatcher_package}
\citation{MM20_SDM}
\citation{MM20_unpub_work}
\citation{imagenet}
\citation{pytorch}
\citation{imagenet}
\citation{osmr}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{3}{section.3}}
\newlabel{sxn:methods}{{3}{3}{Methods}{section.3}{}}
\newlabel{eqn:dnn_energy}{{1}{3}{Methods}{equation.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{DNN Empirical Quality Metrics.}{3}{section*.7}}
\newlabel{eqn:eqn:sum_log_norm}{{4}{3}{DNN Empirical Quality Metrics}{equation.3.4}{}}
\newlabel{eqn:sum_log_alpha_norm_alpha}{{6}{3}{DNN Empirical Quality Metrics}{equation.3.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Convolutional Layers and Normalization issues.}{3}{section*.8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Comparison of CV models}{3}{section.4}}
\newlabel{sxn:cv}{{4}{3}{Comparison of CV models}{section.4}{}}
\citation{pytorch}
\citation{MM20_unpub_work}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces RMSE (smaller is better) for linear fits of quality metrics to reported Top1 test error for pretrained models in each architecture series. Column \# refers to number of models. VGG, ResNet, and DenseNet were pretrained on ImageNet, and ResNet-1K was pretrained on ImageNet-1K. \relax }}{4}{table.caption.12}}
\newlabel{table:cv-models}{{1}{4}{RMSE (smaller is better) for linear fits of quality metrics to reported Top1 test error for pretrained models in each architecture series. Column \# refers to number of models. VGG, ResNet, and DenseNet were pretrained on ImageNet, and ResNet-1K was pretrained on ImageNet-1K. \relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Average Quality Metrics versus Reported Test Accuracies.}{4}{section*.9}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:vgg-fnorm}{{1(a)}{4}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@fig:vgg-fnorm}{{(a)}{4}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{fig:vgg-snorm}{{1(b)}{4}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{sub@fig:vgg-snorm}{{(b)}{4}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\newlabel{fig:vgg-walpha}{{1(c)}{4}{Subfigure 1(c)}{subfigure.1.3}{}}
\newlabel{sub@fig:vgg-walpha}{{(c)}{4}{Subfigure 1(c)\relax }{subfigure.1.3}{}}
\newlabel{fig:vgg-pnorm}{{1(d)}{4}{Subfigure 1(d)}{subfigure.1.4}{}}
\newlabel{sub@fig:vgg-pnorm}{{(d)}{4}{Subfigure 1(d)\relax }{subfigure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of Average Log Norm and Weighted Alpha quality metrics versus reported test accuracy for pretrained VGG models (with and without BN), trained on ImageNet, available in pyTorch (v1.x). Metrics fit by linear regression, RMSE reported. \relax }}{4}{figure.caption.10}}
\newlabel{fig:vgg-metrics}{{1}{4}{Comparison of Average Log Norm and Weighted Alpha quality metrics versus reported test accuracy for pretrained VGG models (with and without BN), trained on ImageNet, available in pyTorch (v1.x). Metrics fit by linear regression, RMSE reported. \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Log Frobenius Norm, VGG }}}{4}{figure.caption.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Spectral Norm, VGG }}}{4}{figure.caption.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces { Weighted Alpha, VGG }}}{4}{figure.caption.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Log $\alpha $-Norm, VGG }}}{4}{figure.caption.10}}
\newlabel{fig:resnet-accuracy}{{2(a)}{4}{Subfigure 2(a)}{subfigure.2.1}{}}
\newlabel{sub@fig:resnet-accuracy}{{(a)}{4}{Subfigure 2(a)\relax }{subfigure.2.1}{}}
\newlabel{fig:resnet1k-accuracy}{{2(b)}{4}{Subfigure 2(b)}{subfigure.2.2}{}}
\newlabel{sub@fig:resnet1k-accuracy}{{(b)}{4}{Subfigure 2(b)\relax }{subfigure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of Avergage $\alpha $-Norm quality metric ($\delimiter "426830A \qopname  \relax o{log}\delimiter "026B30D \mathbf  {X}\delimiter "026B30D _{\alpha }^{\alpha }\delimiter "526930B $) versus reported Top1 test accuracy for the ResNet and ResNet-1K pretrained (pyTorch) models. \relax }}{4}{figure.caption.11}}
\newlabel{fig:cv2-accuracy}{{2}{4}{Comparison of Avergage $\alpha $-Norm quality metric ($\langle \log \Vert \mathbf {X}\Vert _{\alpha }^{\alpha }\rangle $) versus reported Top1 test accuracy for the ResNet and ResNet-1K pretrained (pyTorch) models. \relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { ResNet, Log $\alpha $-Norm }}}{4}{figure.caption.11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { ResNet-1K, Log $\alpha $-Norm }}}{4}{figure.caption.11}}
\@writefile{toc}{\contentsline {paragraph}{Variation in Data Set Size.}{4}{section*.13}}
\citation{resnet1000}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{BouchaudPotters03,SornetteBook,BP11,bun2017}
\citation{MM18_TR,SornetteBook}
\@writefile{toc}{\contentsline {paragraph}{Layer Analysis: Metrics as a Function of Depth.}{5}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{Comparison of VGG, ResNet, and DenseNet Architectures.}{5}{section*.16}}
\newlabel{fig:vgg-alpha-layers}{{3(a)}{5}{Subfigure 3(a)}{subfigure.3.1}{}}
\newlabel{sub@fig:vgg-alpha-layers}{{(a)}{5}{Subfigure 3(a)\relax }{subfigure.3.1}{}}
\newlabel{fig:resnet-alpha-layer}{{3(b)}{5}{Subfigure 3(b)}{subfigure.3.2}{}}
\newlabel{sub@fig:resnet-alpha-layer}{{(b)}{5}{Subfigure 3(b)\relax }{subfigure.3.2}{}}
\newlabel{fig:densenet-alpha-layer}{{3(c)}{5}{Subfigure 3(c)}{subfigure.3.3}{}}
\newlabel{sub@fig:densenet-alpha-layer}{{(c)}{5}{Subfigure 3(c)\relax }{subfigure.3.3}{}}
\newlabel{fig:resnet_alpha_overlaid_depth}{{3(d)}{5}{Subfigure 3(d)}{subfigure.3.4}{}}
\newlabel{sub@fig:resnet_alpha_overlaid_depth}{{(d)}{5}{Subfigure 3(d)\relax }{subfigure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces PL exponent ($\alpha $) versus layer id, for the least and the most accurate models in VGG (a), ResNet (b), and DenseNet (c) series. (VGG is without BN; and note that the Y axes on each plot are different.) Subfigure (d) displays the ResNet models (b), zoomed in to $\alpha \in [1,5]$, and with the layer ids overlaid on the X-axis, from smallest to largest, to allow a more detailed analysis of the most strongly correlated layers. Notice that ResNet152 exhibits different and much more stable behavior of $\alpha $ across layers. This contrasts with how both VGG models gradually worsen in deeper layers and how the DenseNet models are much more erratic. In the text, this is interpreted in terms of \emph  {Correlation Flow}. \relax }}{5}{figure.caption.15}}
\newlabel{fig:3models-alpha-layers}{{3}{5}{PL exponent ($\alpha $) versus layer id, for the least and the most accurate models in VGG (a), ResNet (b), and DenseNet (c) series. (VGG is without BN; and note that the Y axes on each plot are different.) Subfigure (d) displays the ResNet models (b), zoomed in to $\alpha \in [1,5]$, and with the layer ids overlaid on the X-axis, from smallest to largest, to allow a more detailed analysis of the most strongly correlated layers. Notice that ResNet152 exhibits different and much more stable behavior of $\alpha $ across layers. This contrasts with how both VGG models gradually worsen in deeper layers and how the DenseNet models are much more erratic. In the text, this is interpreted in terms of \emph {Correlation Flow}. \relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { VGG }}}{5}{figure.caption.15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { ResNet-1K }}}{5}{figure.caption.15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces { DenseNet }}}{5}{figure.caption.15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces { ResNet-1K (overlaid) }}}{5}{figure.caption.15}}
\@writefile{toc}{\contentsline {paragraph}{Correlation Flow.}{5}{section*.17}}
\citation{CWZZ17_TR}
\citation{distiller}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{Attn2017}
\citation{MM18_TR,MM19_HTSR_ICML}
\citation{MM18_TR,MM19_HTSR_ICML}
\newlabel{fig:resnet204Dmaxev}{{4(a)}{6}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@fig:resnet204Dmaxev}{{(a)}{6}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{fig:resnet204Dalpha}{{4(b)}{6}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@fig:resnet204Dalpha}{{(b)}{6}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ResNet20, distilled with Group Regularization, as implemented in the \texttt  {distiller} (4D\_regularized\_5Lremoved) pretrained models. Log Spectral Norm ($\qopname  \relax o{log}\lambda _{max}$) and PL exponent ($\alpha $) for individual layers, versus layer id, for both baseline (before distillation, green) and fine-tuned (after distillation, red) pretrained models. \relax }}{6}{figure.caption.18}}
\newlabel{fig:resnet204D5L}{{4}{6}{ResNet20, distilled with Group Regularization, as implemented in the \texttt {distiller} (4D\_regularized\_5Lremoved) pretrained models. Log Spectral Norm ($\log \lambda _{max}$) and PL exponent ($\alpha $) for individual layers, versus layer id, for both baseline (before distillation, green) and fine-tuned (after distillation, red) pretrained models. \relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\lambda _{max}$ for ResNet20 layers}}}{6}{figure.caption.18}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha $ for ResNet20 layers}}}{6}{figure.caption.18}}
\@writefile{toc}{\contentsline {paragraph}{Scale Collapse; or How Distillation May Break Models.}{6}{section*.19}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Comparison of NLP Models}{6}{section.5}}
\newlabel{sxn:nlp}{{5}{6}{Comparison of NLP Models}{section.5}{}}
\@writefile{toc}{\contentsline {paragraph}{What do large values of $\alpha $ mean?}{6}{section*.20}}
\citation{huggingface}
\@writefile{toc}{\contentsline {paragraph}{OpenAI GPT Models.}{7}{section*.21}}
\@writefile{toc}{\contentsline {paragraph}{Average Quality Metrics for GPT and GPT2.}{7}{section*.23}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average value for the average Log Norm and Weighted Alpha metrics for pretrainnd OpenAI GPT and GPT2 models. Column \# refers to number of layers treated. Note that the averages do not include the first embedding layer(s) because they are not (implicitly) normalized. \relax }}{7}{table.caption.22}}
\newlabel{table:nlp}{{2}{7}{Average value for the average Log Norm and Weighted Alpha metrics for pretrainnd OpenAI GPT and GPT2 models. Column \# refers to number of layers treated. Note that the averages do not include the first embedding layer(s) because they are not (implicitly) normalized. \relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {paragraph}{Scale Collapse in Poorly Trained Models.}{7}{section*.24}}
\@writefile{toc}{\contentsline {paragraph}{Layer Analysis: Correlation Flow and Scale Collapse in GPT and GPT2.}{7}{section*.26}}
\citation{gpt2-xl}
\newlabel{fig:GPT-alpha-hist}{{5(a)}{8}{Subfigure 5(a)}{subfigure.5.1}{}}
\newlabel{sub@fig:GPT-alpha-hist}{{(a)}{8}{Subfigure 5(a)\relax }{subfigure.5.1}{}}
\newlabel{fig:GPT-snorm-hist}{{5(b)}{8}{Subfigure 5(b)}{subfigure.5.2}{}}
\newlabel{sub@fig:GPT-snorm-hist}{{(b)}{8}{Subfigure 5(b)\relax }{subfigure.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Histogram of PL exponents ($\alpha $) and Log Spectral Norms ($\qopname  \relax o{log}\delimiter "026B30D \mathbf  {W}\delimiter "026B30D _{\infty }$) for weight matrices from the OpenAI GPT and GPT2-small pretrained models.\relax }}{8}{figure.caption.25}}
\newlabel{fig:GPT-hist}{{5}{8}{Histogram of PL exponents ($\alpha $) and Log Spectral Norms ($\log \Vert \mathbf {W}\Vert _{\infty }$) for weight matrices from the OpenAI GPT and GPT2-small pretrained models.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PL exponent ($\alpha $)}}}{8}{figure.caption.25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Spectral Norm ($\qopname \relax o{log}\delimiter "026B30D \mathbf {W}\delimiter "026B30D _{\infty }$)}}}{8}{figure.caption.25}}
\newlabel{fig:gpt-alpha-layer}{{6(a)}{8}{Subfigure 6(a)}{subfigure.6.1}{}}
\newlabel{sub@fig:gpt-alpha-layer}{{(a)}{8}{Subfigure 6(a)\relax }{subfigure.6.1}{}}
\newlabel{fig:gpt-snorm-layer}{{6(b)}{8}{Subfigure 6(b)}{subfigure.6.2}{}}
\newlabel{sub@fig:gpt-snorm-layer}{{(b)}{8}{Subfigure 6(b)\relax }{subfigure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces PL exponents ($\alpha $) (in (a)) and Log Spectral Norms ($\qopname  \relax o{log}\delimiter "026B30D \mathbf  {W}\delimiter "026B30D _{\infty }$) (in (b)) for weight matrices from the OpenAI GPT and GPT2-small pretrained models. (Note that the Y axes on each plot are different.) In the text, this is interpreted in terms of \emph  {Correlation Flow} and \emph  {Scale\nonbreakingspace Collapse}. \relax }}{8}{figure.caption.27}}
\newlabel{fig:gpt-alpha-layers}{{6}{8}{PL exponents ($\alpha $) (in (a)) and Log Spectral Norms ($\log \Vert \mathbf {W}\Vert _{\infty }$) (in (b)) for weight matrices from the OpenAI GPT and GPT2-small pretrained models. (Note that the Y axes on each plot are different.) In the text, this is interpreted in terms of \emph {Correlation Flow} and \emph {Scale~Collapse}. \relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PL exponent ($\alpha $)}}}{8}{figure.caption.27}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Spectral Norm ($\qopname \relax o{log}\delimiter "026B30D \mathbf {W}\delimiter "026B30D _{\infty }$)}}}{8}{figure.caption.27}}
\@writefile{toc}{\contentsline {paragraph}{GPT2: medium, large, xl.}{8}{section*.28}}
\newlabel{fig:gpt2-alpha-hist}{{7(a)}{8}{Subfigure 7(a)}{subfigure.7.1}{}}
\newlabel{sub@fig:gpt2-alpha-hist}{{(a)}{8}{Subfigure 7(a)\relax }{subfigure.7.1}{}}
\newlabel{fig:gpt2-pnorm-hist}{{7(b)}{8}{Subfigure 7(b)}{subfigure.7.2}{}}
\newlabel{sub@fig:gpt2-pnorm-hist}{{(b)}{8}{Subfigure 7(b)\relax }{subfigure.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Histogram of PL exponents ($\alpha $) and Log Alpha Norm ($\qopname  \relax o{log}\delimiter "026B30D \mathbf  {X}\delimiter "026B30D _{\alpha }^{\alpha }$) for weight matrices from models of different sizes in the GPT2 architecture series. (Plots omit the first 2 (embedding) layers, because they are normalized differently giving anamolously large values.) \relax }}{8}{figure.caption.29}}
\newlabel{fig:gpt2-histograms}{{7}{8}{Histogram of PL exponents ($\alpha $) and Log Alpha Norm ($\log \Vert \mathbf {X}\Vert _{\alpha }^{\alpha }$) for weight matrices from models of different sizes in the GPT2 architecture series. (Plots omit the first 2 (embedding) layers, because they are normalized differently giving anamolously large values.) \relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PL exponent ($\alpha $)}}}{8}{figure.caption.29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Alpha Norm}}}{8}{figure.caption.29}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Comparing Hundreds of CV Models}{8}{section.6}}
\newlabel{sxn:all_cv_models}{{6}{8}{Comparing Hundreds of CV Models}{section.6}{}}
\citation{BouchaudPotters03,SornetteBook,BP11,bun2017}
\bibstyle{ACM-Reference-Format}
\bibdata{dnns}
\bibcite{kdd20_sub_repo_anonymized}{{1}{[n.d.]}{{kdd}}{{??}}}
\bibcite{distiller}{{2}{[n.d.]}{{dis}}{{??}}}
\bibcite{gpt2-xl}{{3}{[n.d.]}{{gpt}}{{??}}}
\bibcite{osmr}{{4}{[n.d.]}{{osm}}{{??}}}
\bibcite{weightwatcher_package}{{5}{2018}{{wei}}{{??}}}
\bibcite{BKPx20}{{6}{2020}{{Bahri et~al\unhbox \voidb@x \hbox {.}}}{{Bahri, Kadmon, Pennington, Schoenholz, Sohl-Dickstein, and Ganguli}}}
\bibcite{BFT17_TR}{{7}{2017}{{Bartlett et~al\unhbox \voidb@x \hbox {.}}}{{Bartlett, Foster, and Telgarsky}}}
\bibcite{BouchaudPotters03}{{8}{2003}{{Bouchaud and Potters}}{{Bouchaud and Potters}}}
\bibcite{BP11}{{9}{2011}{{Bouchaud and Potters}}{{Bouchaud and Potters}}}
\bibcite{bun2017}{{10}{2017}{{Bun et~al\unhbox \voidb@x \hbox {.}}}{{Bun, Bouchaud, and Potters}}}
\bibcite{CWZZ17_TR}{{11}{2017}{{Cheng et~al\unhbox \voidb@x \hbox {.}}}{{Cheng, Wang, Zhou, and Zhang}}}
\bibcite{EB01_BOOK}{{12}{2001}{{Engel and den Broeck}}{{Engel and den Broeck}}}
\bibcite{GloBen10}{{13}{2010}{{Glorot and Bengio}}{{Glorot and Bengio}}}
\bibcite{resnet1000}{{14}{2016}{{He et~al\unhbox \voidb@x \hbox {.}}}{{He, Zhang, Ren, and Sun}}}
\bibcite{LMBx18_TR}{{15}{2018}{{Liao et~al\unhbox \voidb@x \hbox {.}}}{{Liao, Miranda, Banburski, Hidary, and Poggio}}}
\bibcite{MM17_TR}{{16}{2017}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{MM18_TR}{{17}{2018}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{MM19_KDD}{{18}{2019a}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{MM19_HTSR_ICML}{{19}{2019b}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{MM20_SDM}{{20}{2020a}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{MM20_unpub_work}{{21}{2020b}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{NTS15}{{22}{2015}{{Neyshabur et~al\unhbox \voidb@x \hbox {.}}}{{Neyshabur, Tomioka, and Srebro}}}
\bibcite{pytorch}{{23}{2019}{{Paszke et~al\unhbox \voidb@x \hbox {.}}}{{Paszke et~al\unhbox \voidb@x \hbox {.}}}}
\bibcite{imagenet}{{24}{2015}{{Russakovsky et~al\unhbox \voidb@x \hbox {.}}}{{Russakovsky et~al\unhbox \voidb@x \hbox {.}}}}
\bibcite{CNNSVD}{{25}{2018}{{Sedghi et~al\unhbox \voidb@x \hbox {.}}}{{Sedghi, Gupta, and Long}}}
\bibcite{SornetteBook}{{26}{2006}{{Sornette}}{{Sornette}}}
\bibcite{Attn2017}{{27}{2017}{{Vaswani et~al\unhbox \voidb@x \hbox {.}}}{{Vaswani et~al\unhbox \voidb@x \hbox {.}}}}
\bibcite{huggingface}{{28}{2019}{{Wolf et~al\unhbox \voidb@x \hbox {.}}}{{Wolf et~al\unhbox \voidb@x \hbox {.}}}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of linear regression fits for different average Log Norm and Weighted Alpha metrics across 5 CV datasets, 17 architectures, covering 108 (out of over 400) different pretrained DNNs. We include regressions only for architectures with five or more data points, and which are postively correlated with test error. These results can be readily reproduced using the Google Colab notebooks (see Appendix\nonbreakingspace \ref  {sxn:appendix}). \relax }}{9}{table.caption.30}}
\newlabel{table:results}{{3}{9}{Comparison of linear regression fits for different average Log Norm and Weighted Alpha metrics across 5 CV datasets, 17 architectures, covering 108 (out of over 400) different pretrained DNNs. We include regressions only for architectures with five or more data points, and which are postively correlated with test error. These results can be readily reproduced using the Google Colab notebooks (see Appendix~\ref {sxn:appendix}). \relax }{table.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{9}{section.7}}
\newlabel{sxn:conc}{{7}{9}{Conclusion}{section.7}{}}
\@writefile{toc}{\contentsline {section}{References}{9}{section*.32}}
\citation{CNNSVD}
\citation{GloBen10}
\citation{pytorch}
\citation{huggingface}
\citation{osmr}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{10}{appendix.A}}
\newlabel{sxn:appendix}{{A}{10}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Reproducibility Considerations}{10}{subsection.A.1}}
\@writefile{toc}{\contentsline {paragraph}{SVD of Convolutional 2D Layers.}{10}{section*.33}}
\@writefile{toc}{\contentsline {paragraph}{Normalization of Empirical Matrices.}{10}{section*.34}}
\@writefile{toc}{\contentsline {paragraph}{Special consideration for NLP models.}{10}{section*.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Reproducing Sections \ref  {sxn:cv} and \ref  {sxn:nlp} }{10}{subsection.A.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Jupyter notebooks used to reproduce all results in Sections\nonbreakingspace \ref  {sxn:cv} and\nonbreakingspace \ref  {sxn:nlp}.\relax }}{10}{table.caption.36}}
\newlabel{table:notebooks}{{4}{10}{Jupyter notebooks used to reproduce all results in Sections~\ref {sxn:cv} and~\ref {sxn:nlp}.\relax }{table.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Reproducing Figure\nonbreakingspace \ref  {fig:resnet204D5L}, for the Distiller Model}{10}{subsection.A.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Reproducing Table\nonbreakingspace \ref  {table:results} in Section\nonbreakingspace \ref  {sxn:all_cv_models} }{10}{subsection.A.4}}
\citation{osmr}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{6.25499pt}
\newlabel{tocindent2}{12.41998pt}
\newlabel{tocindent3}{0pt}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Datasets used\relax }}{11}{table.caption.37}}
\newlabel{table:datasets}{{5}{11}{Datasets used\relax }{table.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Architectures used\relax }}{11}{table.caption.38}}
\newlabel{table:architectures}{{6}{11}{Architectures used\relax }{table.caption.38}{}}
\newlabel{fig:imagenet1k-alpha}{{8(a)}{11}{Subfigure 8(a)}{subfigure.8.1}{}}
\newlabel{sub@fig:imagenet1k-alpha}{{(a)}{11}{Subfigure 8(a)\relax }{subfigure.8.1}{}}
\newlabel{fig:cifar10.alpha}{{8(b)}{11}{Subfigure 8(b)}{subfigure.8.2}{}}
\newlabel{sub@fig:cifar10.alpha}{{(b)}{11}{Subfigure 8(b)\relax }{subfigure.8.2}{}}
\newlabel{fig:cifar100.alpha}{{8(c)}{11}{Subfigure 8(c)}{subfigure.8.3}{}}
\newlabel{sub@fig:cifar100.alpha}{{(c)}{11}{Subfigure 8(c)\relax }{subfigure.8.3}{}}
\newlabel{fig:svhn.alpha}{{8(d)}{11}{Subfigure 8(d)}{subfigure.8.4}{}}
\newlabel{sub@fig:svhn.alpha}{{(d)}{11}{Subfigure 8(d)\relax }{subfigure.8.4}{}}
\newlabel{fig:cub200.alpha}{{8(e)}{11}{Subfigure 8(e)}{subfigure.8.5}{}}
\newlabel{sub@fig:cub200.alpha}{{(e)}{11}{Subfigure 8(e)\relax }{subfigure.8.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces PL exponent $\alpha $ vs. reported Top1 Test Accuracies for pretrained DNNs available for five different data sets. \relax }}{11}{figure.caption.39}}
\newlabel{fig:DSalphas}{{8}{11}{PL exponent $\alpha $ vs. reported Top1 Test Accuracies for pretrained DNNs available for five different data sets. \relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ImageNet 1K}}}{11}{figure.caption.39}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { CIFAR 10 }}}{11}{figure.caption.39}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces { CIFAR 100 }}}{11}{figure.caption.39}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces { SVHN }}}{11}{figure.caption.39}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces { CUB 200 }}}{11}{figure.caption.39}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces $MSE$ Results for all CV model regressions. \relax }}{11}{table.caption.40}}
\newlabel{table:MSEresults}{{7}{11}{$MSE$ Results for all CV model regressions. \relax }{table.caption.40}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces $R^{2}$ Results for all CV model regressions. \relax }}{11}{table.caption.41}}
\newlabel{table:R2results}{{8}{11}{$R^{2}$ Results for all CV model regressions. \relax }{table.caption.41}{}}
\newlabel{TotPages}{{11}{11}{}{page.11}{}}
